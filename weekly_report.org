#+TITLE: Incorporating known risk factors into models
#+DATE: <2017-03-6 Monday>
#+AUTHOR: Jiaxuan Wang
#+EMAIL: jiaxuan@umich
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:nil
#+OPTIONS: todo:t |:t
#+CREATOR: Emacs 24.5.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsmath}


#+BEGIN_LaTeX
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{claim}[1][Claim]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

#+END_LaTeX

* eye properties
   
  #+BEGIN_LaTeX
  \begin{equation} \label{eye-defn}
  eye(x) = \lambda \Big ( \Vert (1-r) \odot \theta \Vert_1 + \sqrt{ \Vert (1-r) \odot \theta \Vert_1^2
  +  \Vert r \odot \theta \Vert_2^2} \Big )
  \end{equation}
  #+END_LaTeX
  
  We want

  1) maintained model performance (perhaps show consistency?)

  2) within a group of dependent features, weights of known risk factors should be
     dense

  3) within a group of dependent features of all unknown risk factors, the weights
     should be sparse
     
  We have

  1) eye is a norm
  2) eye is \beta free
  3) eye is a generalization of lasso, ridge, and elastic net

* equivalence with original definition

  recall the original eye definition is 

     #+BEGIN_LaTeX
     \begin{equation} \label{eye-defn-orig}
     eye(x) = \lambda \inf\{t>0 \mid x \in \{t x \mid q(x) = \frac{\beta^2}{1-\beta}\}\}
     \end{equation}

     where $q(\theta) = 2 \beta  \Vert (1-r) \odot \theta \Vert_1 + 
     (1-\beta)  \Vert r \odot \theta \Vert_2^2$
     #+END_LaTeX     
     
     #+BEGIN_proof
     
     Since $\beta$ can be arbitrarily set, fix $\beta$=0.5, 
     
     then eqref:eye-defn-orig becomes
     
     \begin{equation}
     eye(x) = \lambda \inf\{t>0 \mid x \in t \{ x \mid 
     2  \Vert (1-r) \odot x \Vert_1 +  \Vert r \odot x \Vert_2^2 = 1\}\}
     \end{equation}
     
     Assume $x \neq 0$ and denote $eye(x) := \lambda t$, then $x \in t \{ x \mid 
     2  \Vert (1-r) \odot x \Vert_1 +  \Vert r \odot x \Vert_2^2 = 1\}$, that is 
     $\frac{2 \Vert (1-r) \odot x \Vert_1}{t} + \frac{ \Vert r \odot x \Vert_2^2}{t^2} = 1$
     
     As this is a quadratic equation in t and from assumption we known t>0 (eye
     being a norm and $x \neq 0$), solving for t yields:
     
     #+BEGIN_LaTeX
     \begin{equation} \label{tmp-derivation}
     t = \Vert (1-r) \odot x \Vert_1 + 
     \sqrt{ \Vert (1-r) \odot x \Vert_1^2 +  \Vert r \odot x \Vert_2^2}
     \end{equation}
     #+END_LaTeX
     
     Note that in the event $x=0$, t=0, agreeing with the fact that $eye(0)=0$.
     Thus eqref:eye-defn and eqref:eye-defn-orig are equivalent.
     
     #+END_proof

* orthogonal regression case
   We consider a special case of regression and orthogonal design matrix (X^T X
   = I) with eye regularization. This restriction allows us to obtain a closed
   form solution so that key features of eye penalty can be highlighted.
   
   With eqref:eye-defn, we have
  
   #+BEGIN_LaTeX
   \begin{equation} \label{regression-obj}
   \min_{\theta} \frac{1}{2} \Vert y - X \theta \Vert_2^2 + n \lambda 
   \Big ( \Vert (1-r) \odot \theta \Vert_1 + 
   \sqrt{\Vert (1-r) \odot \theta \Vert_1^2 + 
    \Vert r \odot \theta \Vert_2^2} \Big )
   \end{equation}
   
   Since the objective is convex, we solve for its subgradient g.
   
   \begin{equation}  \label{orthog-general}
   g = X^T X \theta - X^T y + n \lambda (1-r) \odot s + 
   \frac{n\lambda}{Z} (\Vert (1-r) \odot \theta \Vert_1 (1-r) \odot s +  r \odot r \odot \theta)
   \end{equation}
   
   where $s_i = sgn(\theta_i)$ if $\theta_i \neq 0$, 
   $s_i \in [-1,1]$ if $x_i =0$, and
   $Z = \sqrt{\Vert (1-r) \odot \theta \Vert_1^2 + 
    \Vert r \odot \theta \Vert_2^2}$.
   
   By our assumption $X^T X = I$, and the fact that 
   $\hat \theta^{OLS} = (X^T X)^{-1} X^T y = X y$
   (the solution for oridinary least square), we simplify \ref{orthog-general}
   as 

   \begin{equation}
   g = \theta - \hat \theta^{OLS} + n \lambda (1-r) \odot s + 
   \frac{n \lambda}{Z} (\Vert (1-r) \odot \theta \Vert_1 (1-r) \odot s + r \odot r \odot \theta)
   \end{equation}
   
   setting g to 0 we have
   
   \[
   \hat \theta_i = 
   \begin{cases}
   \frac{\hat \theta_i^{OLS}}{1+\frac{n \lambda}{Z} r_i^2} - 
   \frac{n \lambda (1-r_i)(1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z})}{1+\frac{n\lambda}{Z} r_i^2},
   & \text{if } \hat \theta_i^{OLS} > n \lambda (1-r_i)(1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z})\\
   0,& \text{if } |\hat \theta_i^{OLS}| < n \lambda (1-r_i)(1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z})\\
   \frac{\hat \theta_i^{OLS}}{1+\frac{n \lambda}{Z} r_i^2} +
   \frac{n \lambda (1-r_i)(1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z})}{1+\frac{n\lambda}{Z} r_i^2}
   ,& \text{otherwise}
   \end{cases}
   \]
   
   in more compact notation
   \begin{equation} \label{orthog-theta}
   \hat \theta_i = \frac{\hat \theta_i^{OLS}}{1+\frac{n \lambda}{Z} r_i^2}
   \max \Big ( 0, 
   1-\frac{n \lambda (1-r_i)(1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z})}{|\hat \theta_i^{OLS}|} \Big )
   \end{equation}

   where $Z = \sqrt{\Vert (1-r) \odot \hat \theta \Vert_1^2 + 
    \Vert r \odot \hat \theta \Vert_2^2}$.

   note that \ref{orthog-theta} is still an implicit equation in $\theta$
   because $Z$ is a function of $\hat \theta$. Also we implicitly assumed that $Z \neq 0$.
   #+END_LaTeX
   
   It may worry you if $Z=0$: wouldn't the denominator be 0? This only happens
   if $\theta=0$. However, by the complimentary slackness condition in KKT, we
   know $\lambda>0$ implies solution is on the boundary of the constraint
   formulation of the problem (for $\lambda=0$, we are back to ordinary least
   square, there's no point in discussing that). So long as the optimal solution
   for the unconstrained problem is not at *0*, we won't get into trouble unless
   the constraint is $eye(\theta) \leq 0$, which won't happen in the regression
   setting as $\lambda$ is finite. If the optimal solution for the unconstrained
   problem is *0*, we are again back to ordinary least square solutions. So the
   upshot is we can assume $Z \neq 0$ otherwise it will automatically fall back
   to ordinary least square.
   
   [[./soft_threshold.png]]
  
** TODO The slope between $\hat \theta_i$ and $\hat \theta_i^{OLS}$ is non-negative
   
   For $r_i=0$ or $r_i=1$, the function is monotone non-decreasing.

   For fractional r, evidence abound (actually not right, need further check as
   $\theta_i$ depends on $\theta_j$ for all $j$)
   [[./symdiff.ipynb]]
   
   The proof will be messy (don't know if worth it)

   + The messy proof for $\hat \theta_i^{OLS} \geq 0$
     
     TO FILL IN
   
   The other part of the proof follows by observing that relation between $\hat
   \theta_i$ and $\hat \theta_i^{OLS}$ is symmetric about the origin.
   
* TODO perfectly correlated case
  
  Denote the objective function in \ref{regression-obj} as $L(\theta)$. 
  Assume $\hat \theta$ is the optimal solution, $x_i = x_j$
  (e.g. the $i^{th}$ and $j^{th}$ columns of design matrix is co-linear)

  + r_i = 1, r_j = 0, x_i = x_j => $\hat \theta_j = 0$
    
    Here we show eye penalty prefers known risk factors over unknown risk factors.

    #+BEGIN_LaTeX
    \begin{proof}
    Assume $r_i=1$, $r_j=0$.
    
    consider $\theta'$ that only differs from $\hat \theta$ at the $i^{th}$ and $j^{th}$
    entry such that $\theta'_i = \hat \theta_i + \hat \theta_j$ and $\theta'_j=0$.
    
    $L(\hat \theta) - L(\theta') = \frac{1}{2} \Vert y-X \hat \theta\Vert_2^2 +
    n\lambda \Big ( | \hat \theta_j | + \sqrt{(C+
    | \hat \theta_j |)^2 + D + \hat \theta_i^2} \Big) 
    - \frac{1}{2} \Vert y-X \theta' \Vert_2^2 - n\lambda \Big( | \theta_j' | + \sqrt{(C+
    | \theta_j' |)^2 + D + \theta_i'^2} \Big )$
    
    where C and D are nonnegative constant involving entries other than $i$ and $j$. 
    Note that the sum of squared residue is the same for both 
    $\theta'$ and $\hat \theta$ owing to the fact that
    $x_i=x_j$. Along with definition of $\theta'$, we have
    
    \begin{align*} 
    L(\hat \theta) - L(\theta') &= n\lambda \Big ( | \hat \theta_j | + \sqrt{(C+
    | \hat \theta_j |)^2 + D + \hat \theta_i^2} 
    - \sqrt{C^2 + D + (\hat \theta_i + \hat \theta_j)^2} \Big )
    \end{align*}
    
    \begin{claim} \label{claim1}
    $L(\hat \theta) - L(\theta') \geq 0$ with equality only if $\hat \theta_j=0$
    \end{claim}
    
    \begin{proof}
    Since $n\lambda$ is positive, the claim is equivalent to
    $$ \sqrt{(C+ | \hat \theta_j |)^2 + D + \hat \theta_i^2} 
    \geq \sqrt{C^2 + D + (\hat \theta_i + \hat \theta_j)^2} - | \hat \theta_j |$$
    
    If right hand side is negative, we are done as left hand side is nonnegative.
    
    Othewise, both sides are nonnegative, we square them and rearrange to get the 
    equivalent form
    
    $$\hat \theta_j^2 + 2 \hat \theta_i \hat \theta_j \leq 2 |\hat \theta_j| \sqrt{C^2+D+(\hat \theta_i + \hat \theta_j)^2} + 2 C |\hat \theta_j|$$
    
    which is true following
  
    \begin{align}
    \hat \theta_j^2 + 2 \hat \theta_i \hat \theta_j &\leq 2\hat \theta_j^2 + 2 \hat \theta_i \hat \theta_j - \hat \theta_j^2 \label{sq-drop1}\\
    &\leq 2|\hat \theta_j||\hat \theta_i+\hat \theta_j| \label{sq-drop2} \\
    &= 2|\hat \theta_j|\sqrt{(\hat \theta_i+\hat \theta_j)^2}\\
    &\leq 2 |\hat \theta_j| \sqrt{C^2+D+(\hat \theta_i + \hat \theta_j)^2} + 2 C |\hat \theta_j|
    \end{align}
    
    Again if $\hat \theta_j \neq 0$, the inequality is strict from \eqref{sq-drop1} to \eqref{sq-drop2}
  
    \end{proof}
  
    Since we assumed that $\hat \theta$ is optimal, the equality in \ref{claim1} must hold, thus $\hat \theta_j=0$.
  
    \end{proof}
    #+END_LaTeX
  + r_i = 1, r_j = 1, x_i = x_j => $\hat \theta_i = \hat \theta_j$

    Feature weights are dense in known risk factors

    #+BEGIN_LaTeX
    \begin{proof}
    Assume $\hat \theta$ is optimal, consider $\theta'$ that is the same
    as $\hat \theta$ except $\theta'_i = \theta'_j= \frac{\hat \theta_j + \hat \theta_j}{2}$.

    Assume $\hat \theta \neq \theta'$: $\hat \theta_i \neq \hat \theta_j$. 
    Again the sum of residue of for both estimation is unchanged as $x_i=x_j$
    
    \begin{align*}
    L(\hat \theta) - L(\hat \theta) &= n\lambda \Big ( \sqrt{(C+|\hat \theta_i|+|\hat \theta_j|)^2
    +D+\hat \theta_i^2 + \hat \theta_j^2} - 
    \sqrt{(C+2\frac{|\hat \theta_i + \hat \theta_j|}{2})^2 + D+ 2 \frac{|\hat \theta_i + \hat \theta_j|^2}{4}} \Big)\\
    &\geq n\lambda \Big ( \sqrt{(C+|\hat \theta_i|+|\hat \theta_j|)^2
    +D+\hat \theta_i^2 + \hat \theta_j^2} - 
    \sqrt{(C+|\hat \theta_i| + |\hat \theta_j|)^2 + D+ \frac{|\hat \theta_i + \hat \theta_j|^2}{2}} \Big)\\
    \end{align*}

    Since $$\hat \theta_i^2 + \theta_j^2 - \frac{|\hat \theta_i + \hat \theta_j|^2}{2} = \frac{(\hat \theta_i - \hat \theta_j)^2}{2}>0$$
    by assumption that $\hat \theta_i \neq \hat \theta_j$ for the optimal solution. This shows $L(\hat \theta) - L(\hat \theta)>0$,
    which contradict our assumption.
    
    Thus $\hat \theta_i=\hat \theta_j$ for the optimal solution.
    \end{proof}
    #+END_LaTeX
  + r_i = 0, r_j = 0, x_i = x_j => back to lasso continuum 
    
    Note that fixing $\theta_k$ $\forall k \not \in \{i,j\}$, solving for
    $\theta_i$ and $\theta_j$ reduces the problem for 2 dimensional lasso 
    regression, thus all properties of lasso carry over for $\theta_i$ and
    $\theta_j$. Thus sparsity is maintained in unknown features.
  + r_i > r_j and $sgn(\hat \theta_i) > 0$ => $\hat \theta_i > \hat \theta_j \geq 0$
    
    TODO: fill in the already proved proof
    2 parts, one to say that $sgn(\theta_i)=sgn(\theta_j)$, the other
    show the main claim
  + r_i > r_j and $sgn(\hat \theta_i) < 0$ => $\hat \theta_i < \hat \theta_j
    \leq 0$
    
    Follow the same proof as the above except flip sign at the last step

** TODO general case with diferent r_i and r_j
* general correlation

  grouping effect in elastic net is still present in eye penalty within
  groups with similar level of risk.

  #+BEGIN_LaTeX
  \begin{theorem}
  if $\hat \theta_i \hat \theta_j > 0$ and design matrix is standardized, then
  \begin{equation*}
  \frac{|r_i^2 \hat \theta_i - r_j^2 \hat \theta_j|}{Z} \leq \frac{\sqrt{2 (1-\rho)} \Vert y \Vert_2}{n\lambda}
  + |r_i-r_j| (1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z})
  \end{equation*}

  where $Z = \sqrt{\Vert (1-r) \odot \hat \theta \Vert_1^2 + \Vert r \odot \hat \theta \Vert_2^2}$,
  $\rho$ is the sample covariance between $x_i$ and $x_j$
  \end{theorem}

  \begin{proof}
  Denote the objective in \ref{regression-obj} as $L$. Assume $\hat \theta_i \hat \theta_j > 0$, 
  $\hat \theta$ is the optimal weights, and the design matrix $X$ is standardized to have zero mean and unit
  variance in its column. Via the optimal condition and \ref{orthog-general}, $g(\hat \theta)=0$. 
  Hence we have
  
  \begin{align}
  -x_i^T(y-X\hat \theta) + n\lambda((1-r_i) s_i + \frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z}
  ((1-r_i) s_i + r_i^2 \hat \theta_i)) \label{corr-eq1}
  \\
  -x_j^T(y-X\hat \theta) + n\lambda((1-r_j) s_j + \frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z}
  ((1-r_j) s_j + r_j^2 \hat \theta_j)) \label{corr-eq2}
  \end{align}

  Substract \ref{corr-eq2} from \ref{corr-eq1}. The assumption that $\hat \theta_i \hat \theta_j > 0$ 
  implies $sgn(\hat \theta_i)=sgn(\hat \theta_j)$ and eliminates the need to discuss subgradient issue.
  
  $$(x_j^T-x_i^T)(y-X\hat \theta) + n\lambda((r_j-r_i)sgn(\hat \theta_i) + \frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z}
  ((r_j-r_i) sgn(\hat \theta_i) + r_i^2 \hat \theta_i - r_j^2 \hat \theta_j)) = 0$$

  Rearange to get

  \begin{equation} \label{corr-eq3}
  \frac{r_i^2 \hat \theta_i - r_j^2 \hat \theta_j}{Z} = \frac{(x_i^T - x_j^T)(y-X\hat \theta)}{n \lambda}
  + (r_i-r_j)sgn(\hat \theta_i) \Big( 1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z} \Big )
  \end{equation}
  
  Being the optimal weights, $L(\hat \theta) \leq L(\textbf{0})$, which implies
  $\Vert y-X\hat \theta \Vert_2^2 \leq \Vert y \Vert_2^2$

  Also, standardized design matrix gives $\Vert X_i-X_j\Vert_2^2=<x_i, x_i> + <x_j, x_j> - 2<x_i, x_j>=2(1-\rho)$
  
  Taking the absolute value of \ref{corr-eq3} and applying cauchy schwarz inequality, we get
  \begin{align}
  \frac{|r_i^2 \hat \theta_i - r_j^2 \hat \theta_j|}{Z} &\leq \frac{\Vert x_i - x_j \Vert_2 \Vert y-X\hat \theta \Vert_2}{n \lambda}
  + |r_i - r_j| (1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z})\\
  &\leq \frac{\sqrt{2(1-\rho)}\Vert y \Vert_2}{n \lambda}
  + |r_i - r_j| (1+\frac{\Vert (1-r) \odot \hat \theta \Vert_1}{Z})
  \end{align}
  
  \end{proof}

  \begin{corollary}
  if $\hat \theta_i \hat \theta_j > 0$, design matrix is standardized, and $r_i=r_j \neq 0$
  $$\frac{|\hat \theta_i - \hat \theta_j|}{Z} \leq \frac{\sqrt{2(1-\rho) \Vert y \Vert_2}}{r_i^2 n \lambda}$$

  where $Z = \sqrt{\Vert (1-r) \odot \hat \theta \Vert_1^2 + \Vert r \odot \hat \theta \Vert_2^2}$,
  $\rho$ is the sample covariance between $x_i$ and $x_j$
  \end{corollary}
   
  This verifies the existence of grouping effect: highly correlated weights with similiar risk 
  are grouped together in weight space.
  #+END_LaTeX

* analysis on run 6 and 7

  run 6 results: (sweep correlation)

  [[./figures/sample_figure_corr.png]]

  [[./run6.numbers]]
  
  run 7 results: (sweep fractoinal r)

  [[./run7.numbers]]
  
  [[./run7_analysis.ipynb]]
* TODO [#B] literature review on interpretation
* TODO [#B] different theta experiment: make a regression problem
* TODO [#B] consistent estimator: rate of convergence
* TODO [#B] general linearly independent case
* TODO next
1. proof general r
2. run 3 shapes with lots of features per group
