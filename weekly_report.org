#+TITLE: Incorporating known risk factors into models
#+DATE: <2017-02-14 Tue>
#+AUTHOR: Jiaxuan Wang
#+EMAIL: jiaxuan@umich
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:nil
#+OPTIONS: todo:t |:t
#+CREATOR: Emacs 24.5.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* objective
Incorporating known risk factors with unknown risk factors in predicting outcome. 
In the case of choosing between correlated variables, the model should favor
known risk factors.

* approaches taken

** old approach 

0.5 * \lambda_2 ||r * \theta||_2^2 + \lambda_1 ||(1-r) * \theta||_1
where r \in \{0,1\}^d, \theta \in R^d

[[./contour/penalty.png]]

** new approach

Fix a convex body that have property of the previous contour plot such that the
angle at the end point is 45 degree. The following is the contour plot of its
induced norm

[[./contour/eye.png]]

In fact, by relaxing the constraint of r over binary to float, we can recover
enet (setting r=0.5 * 1). Even without extending r, we can recover ridge (r=1) 
and lasso (r=0)

[[./contour/eye_enet.png]]

[[./contour/eye_ridge.png]]

[[./contour/eye_lasso.png]]

* experiments
** set up
Data n=100:

[[./figures/data.png]]

h = linspace(-2.5, 1, n)

x_0 ~ Uniform(1..4) h + N(0, 0.2)

x_1 ~ Uniform(1..4) h + N(0, 0.2)

Loss function is the negative loss likelihood of the logistic regression model.

Optimizer: AdaDelta

Number of Epoch: 1000

Regulizers: elastic net, lasso, ridge, OWL, weighted lasso, weighted ridge,
penalty, eye penalty

*** eye penalty

pena(\theta) = \alpha * (0.5 * (1-c) * ||r * \theta||_2^2 + c * ||(1-r) *
\theta||_1)

where r \in [0,1]^d, \theta \in R^d, \alpha \in R, c \in R

unit ball defined as:

pena(\theta) = k

where k is chosen so that slope in the first quadrant between known risk
factor x and unknown risk factor is -1

eye_penalty(\theta) = |t| s.t pena(\theta/t) = k if \theta \neq 0 else 0

This is indeed a norm

1) eye_penalty(t \theta) = |t| eye_penalty(\theta)
2) eye_penalty(\theta + \beta) \leq eye_penalty(\theta) + eye_penalty(\beta)
3) eye_penalty(\theta) = 0 iff \theta = 0


varying r_1 and r_2

r_1 = 0.0

[[./contour/eye_0_0.png]]

r_1 = 0.1

[[./contour/eye_0.1.png]]

r_1 = 0.2

[[./contour/eye_0.2.png]]

r_1 = 0.3

[[./contour/eye_0.3.png]]

r_1 = 0.4

[[./contour/eye_0.4.png]]

r_1 = 0.5

[[./contour/eye_0.5.png]]

r_1 = 0.6

[[./contour/eye_0.6.png]]

r_1 = 0.7

[[./contour/eye_0.7.png]]

r_1 = 0.8

[[./contour/eye_0.8.png]]

r_1 = 0.9

[[./contour/eye_0.9.png]]

r_1 = 1.0

[[./contour/eye_1.0.png]]

*** elastic net
\alpha * (c * ||\theta||_1 + 0.5 * (1 - c) * ||\theta||_2^2) where c is a scaler

[[./contour/enet_add.png]] 

*** lasso
\alpha * ||\theta||_1

[[./contour/lasso_add.png]]

*** ridge
0.5 * \alpha * ||\theta||_2^2

[[./contour/ridge_add.png]]

*** OWL
\alpha * \sum_{i=1}^n w_i |x|_{[i]} where w \in K_{m+} (monotone nonnegative cone)

[[./contour/OWL_w1=2>w2=1.png]]

degenerated case: back to lasso

[[./contour/OWL_w1=1=w2=1.png]]

degenerated case: back to l_{\inf}

[[./contour/OWL_w1=2>w2=0.png]]

some properties:

generalization of OSCAR norm

symmetry with respect to signed permutations

in the regular case, the minimal atomic set for this norm is known (the corners
are easily calculated)

*** weighted lasso
\alpha * ||w * \theta||_1 where w \in R_+^d

[[./contour/wlasso_add.png]]

*** weighted ridge 
0.5 * \alpha * ||w * \theta||_2^2 where w \in R_{+}^{d}

[[./contour/wridge_add.png]]

*** old penalty
\alpha * (0.5 * (1-c) * ||r * \theta||_2^2 + c * ||(1-r) * \theta||_1)
where r \in \{0,1\}^d, \theta \in R^d, \alpha \in R, c \in R

[[./contour/penalty_add.png]]







** running procedure

*** first run

b regularized

fix hyperparmeters to predefined value

repeat the following 100 times:

generate data, run the selected regularizers, record \theta

[[./old_figures/$x_0$_distribution.png]]

[[./old_figures/$x_1$_distribution.png]]

[[./old_figures/b_distribution.png]]

[[./old_figures/avg_reg.png]]

*** second run

b unregularized

generate two datasets, one for training, one for validation

parameter search over the different hyperparams of the regularizers

for each regularizer, use the hyperparmeters that acheives the minimal loss

repeat the following 100 times:

generate data, run the selected regularizers, record \theta

*** third run

b unregularized

generate two datasets, one for training, one for validation

normalize the data to 2 mean and 2 variance (validaton data is normalized
using mean and variance for the training data)

parameter search over the different hyperparams of the regularizers

for each regularizer, use the hyperparmeters that acheives the minimal loss

repeat the following 100 times:

generate data, normalize data, run the selected regularizers, record \theta

The choosing criteria is still loss b/c AUROC is always going to be 1 in the
deterministic case:

[[./figures/$x_0$_distribution.png]]

[[./figures/$x_1$_distribution.png]]

[[./figures/$x_2$_distribution.png]]

[[file:figures/avg_reg.png]]
